LOAD CSV WITH HEADERS FROM "file:///nodes_to_graph_id.csv" AS csvLine1
MERGE (u:User
 { 
    node_id: coalesce(csvLine1.user_node_id, "No user_node_id"),
    graph_id: coalesce(csvLine1.graph_id, "No graph_id"),
    label: coalesce(csvLine1.label, "No label")
 }
);

LOAD CSV WITH HEADERS FROM "file:///edges.csv" AS csvLine2
MATCH (u1:User { node_id: csvLine2.from })
MATCH (u2:User { node_id: csvLine2.to })
MERGE (u1)-[:retweeted_post_of]->(u2)

// verify datasets, mean should be like in GNN-fake-news-detection on github

// create graph projection
CALL gds.graph.project('retweet_network','User','retweeted_post_of');

CALL gds.wcc.stats('retweet_network')
YIELD componentCount, componentDistribution
RETURN componentCount, 
       componentDistribution.min as min,
       componentDistribution.max as max,
       componentDistribution.mean as mean


// How many users with more than 1 relation

MATCH (u:User)
WHERE size((u)-[:retweeted_post_of]-()) > 1
RETURN u

// compute betweenness for true labeled nodes

// CALL gds.betweenness.stream('retweet_network')
// YIELD nodeId, score
// MATCH (user: User { label: "1.0"} ) WHERE id(user) = nodeId
// RETURN user.node_id, score
// ORDER BY score DESC

// export betweenness correlation data

CALL apoc.export.csv.query("
CALL gds.betweenness.stream('retweet_network')
YIELD nodeId, score
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, score as betweenness_score
ORDER BY nodeId ASC", 'betweenness_correlation_data.csv', {});

// export closeness correlation data Wasserman and Faust

CALL apoc.export.csv.query("
CALL gds.beta.closeness.stream('retweet_network', {useWassermanFaust: true})
YIELD nodeId, score
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, score as closeness_score
ORDER BY nodeId ASC", 'closeness_correlation_data.csv', {})


// export harmonic closeness

CALL apoc.export.csv.query("
CALL gds.alpha.closeness.harmonic.stream('retweet_network')
YIELD nodeId, centrality
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, centrality as harmonic_closeness_centrality
ORDER BY nodeId ASC", 'harmonic_closeness_correlation_data.csv', {})

// export degree correlation data

CALL apoc.export.csv.query("MATCH (u:User)
WITH id(u) as nodeId, size((u)-[:retweeted_post_of]->()) as outDegree, size((u)<-[:retweeted_post_of]-()) as inDegree
MATCH (u: User) WHERE id(u) = nodeId
RETURN u.label, outDegree, inDegree, inDegree + outDegree as degree
ORDER BY nodeId ASC", 'degree_correlation_data.csv', {})

// PageRank

CALL apoc.export.csv.query("CALL gds.pageRank.stream('retweet_network', {maxIterations: 20, dampingFactor: 0.85})
YIELD nodeId, score
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, score as page_rank_score
ORDER BY nodeId ASC", 'page_rank_correlation_data.csv', {})

// ArticleRank
CALL apoc.export.csv.query("CALL gds.articleRank.stream('retweet_network', {maxIterations: 20, dampingFactor: 0.85})
YIELD nodeId, score
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, score as page_rank_score
ORDER BY nodeId ASC", 'article_rank_correlation_data.csv', {})


// Eigenvector
CALL apoc.export.csv.query("CALL gds.eigenvector.stream('retweet_network', {maxIterations: 40})
YIELD nodeId, score
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, score as eigenvector_score
ORDER BY nodeId ASC", 'eigenvector_correlation_data.csv', {})

// HITS
CALL apoc.export.csv.query("CALL gds.alpha.hits.stream('retweet_network', {hitsIterations: 50})
YIELD nodeId, values
MATCH (user: User) WHERE id(user) = nodeId
RETURN user.label, values.hub as hits_hub, values.auth as hits_auth
ORDER BY nodeId ASC", 'hits_correlation_data.csv', {})


LOAD CSV WITH HEADERS FROM "file:///tweets_processed.csv" AS csvLine
MERGE (u:User
 { 
    twitter_id: coalesce(csvLine.user_id, "unknown")
 }
)
MERGE (t: Tweet
 { 
    twitter_id: coalesce(csvLine.id, "unknown"),
    text: coalesce(csvLine.id, "unknown"),
    source: coalesce(csvLine.source, "unknown"),
    createdAt: coalesce(apoc.date.parse(csvLine.timestamp, "ms", "yyyy-MM-dd HH:mm:ss"), "unknown"),
    possibly_sensitive: coalesce(csvLine.possibly_sensitive, "unknown"),
    place: coalesce(csvLine.place, "unknown")
 }
)
MERGE (t)-[:postedBy]->(u)



LOAD CSV WITH HEADERS FROM "file:///tweets_processed.csv" AS csvLine
MERGE (u:User
 { 
    twitter_id: coalesce(csvLine.user_id, "unknown")
 }
)
MERGE (t: Tweet
 { 
    twitter_id: coalesce(csvLine.id, "unknown"),
    text: coalesce(csvLine.id, "unknown"),
    source: coalesce(csvLine.source, "unknown"),
    createdAt: coalesce(apoc.date.parse(csvLine.timestamp, "ms", "yyyy-MM-dd HH:mm:ss"), "unknown"),
    possibly_sensitive: coalesce(csvLine.possibly_sensitive, "unknown"),
    place: coalesce(csvLine.place, "unknown")
 }
)
MERGE (t)-[:postedBy]->(u)


dbms.memory.heap.initial_size=1G
dbms.memory.heap.max_size=10G
dbms.memory.pagecache.size=1G


CREATE CONSTRAINT FOR (u:User) REQUIRE u.twitterId IS UNIQUE;
CREATE CONSTRAINT FOR (t:Tweet) REQUIRE t.twitterId IS UNIQUE;


:auto LOAD CSV WITH HEADERS FROM "file:///tweets_processed.csv" AS csvLine
CALL {
    
WITH csvLine
MERGE (u:User { twitterId: csvLine.user_id })
CREATE (t: Tweet
 {  twitterId: csvLine.id,
    text: coalesce(csvLine.text, "unknown"),
    source: coalesce(csvLine.source, "unknown"),
    createdAt: coalesce(date(datetime({ epochmillis: 
apoc.date.parse(csvLine.timestamp, "ms", "yyyy-MM-dd HH:mm:ss") })), "unknown"),
    possibly_sensitive: coalesce(csvLine.possibly_sensitive, "unknown"),
    place: coalesce(csvLine.place, "unknown")
 }
)
CREATE (t)-[:postedBy]->(u)
} IN TRANSACTIONS OF 50000 ROWS


// MATCH (t: Tweet) DETACH DELETE t;
// MATCH (u: User) DETACH DELETE u;

LOAD CSV WITH HEADERS FROM "file:///tweets_processed.csv" AS csvLine
WITH csvLine
MERGE (t2: Tweet { twitterId: csvLine.in_reply_to_status_id});


LOAD CSV WITH HEADERS FROM "file:///tweets_processed.csv" AS csvLine
WITH csvLine
MERGE (t3: Tweet { twitterId: csvLine.retweeted_status_id});


:auto LOAD CSV WITH HEADERS FROM "file:///tweets_processed.csv" AS csvLine
CALL {
WITH csvLine
MATCH (t1: Tweet { twitterId: csvLine.id })
MATCH (t2: Tweet { twitterId: csvLine.in_reply_to_status_id})
MATCH (t3: Tweet { twitterId: csvLine.retweeted_status_id})

MERGE (t1)-[:inReplyToStatus { screenName: coalesce(csvLine.in_reply_to_screen_name, 'unknown') }]->(t2)
MERGE (t1)-[:retweetedStatus]->(t3)
} IN TRANSACTIONS OF 50000 ROWS

:auto CALL {
    MATCH (t1: Tweet) WHERE t1.twitterId = '0' DETACH DELETE t1
} IN TRANSACTIONS OF 10000 ROWS

:sysinfo

## with gossipcop
:auto LOAD CSV WITH HEADERS FROM "file:///nodes_to_graph_id.csv" AS csvLine1
CALL {
WITH csvLine1
MERGE (u:User
 { 
    node_id: coalesce(csvLine1.user_node_id, "No user_node_id")
 })
ON CREATE SET u.graph_id = coalesce(csvLine1.graph_id, "No graph_id"), u.label = coalesce(csvLine1.label, "No label")
} IN TRANSACTIONS OF 10000 ROWS;

:auto LOAD CSV WITH HEADERS FROM "file:///edges.csv" AS csvLine2
CALL {
WITH csvLine2
MATCH (u1:User { node_id: csvLine2.from })
MATCH (u2:User { node_id: csvLine2.to })
MERGE (u1)-[:retweeted_post_of]->(u2)
} IN TRANSACTIONS OF 10000 ROWS;